The main challenge of this project for both the visualization and analysis was the high dimensionality of the data set. Building an application that can display information relating to each of the different aspects of the data was a task that required researching the domain in order to construct meaningful visualizations and then researching techniques that facilitate the implementation of these visualizations through a suitable platform. This was an iterative process that required refinement and revision at each step. The code found at \cite{ShinyServer} documents the order in which these revisions and improvements were carried out and is reflective of the chronological development of the App.
\hfill\break
\newline
Building an App that implemented the desired functionality but was also responsive, user friendly, extendable and scalable was a complex problem to tackle. From the outset of the development phase, I broke up this complex problem into many smaller, simpler sub-problems, while simultaneously designing an architecture for the App which could efficiently recombine the solutions to each of these sub-problems. This manifested itself as the modularization of the App into tabs which dealt with different aspects of the data separately. This meant that the App could be developed in incremental steps where editing, removing or adding a tab was independent of the existence of other tabs in the App. 
\hfill\break
\newline
The other main challenge in the development of the App was efficiently manipulating the data in order to display some aspect of the data. Complex filtering, subsetting, aggregating, splitting, mapping and recombining of the data was necessary to display most of the graphs in the App. These operations were dependent on user input, which added a further layer of complexity. The R functions apply, aggregate, paste, match, rbind, unique, ifelse, Reduce and append were used extensively to implement these operations. Packages such as TidyR could be explored in future to avoid the necessity of repeating these operations for multiple graphs by reshaping the data to a more convenient form as soon as it is read in by the App. Further work on this project could include deployment of the App to a server, improving the validation and error-checking/handling mechanisms, re-writing portions of the code to facilitate the usage of data sets from different sports and optimizing the responsiveness of the App.
\hfill\break
\newline
The statistical analysis of the New.Bodyload variable was also complicated by the high number of dimensions in the data. The ideal model was one that had few variables but also explained a high proportion of the variance relating to New.Bodyload, therefore a large part of the analysis focused on reducing the number of variables required in the statistical models. Multicollinearity of the predictors complicated this process, solutions to which were explored through orthogonalization techniques such as PCA and Partial Least Squares.
\hfill\break
\newline
The problem of identifying outliers in the data also consumed a large part of the time dedicated to the statistical analysis. As discussed in 2.4.1, I was not able to conclusively determine whether the suspected erroneous points were in fact erroneous, but I was able to suggest explanations for the generation of these points. Further analysis could have been performed on the process that generated these points, giving deeper meaning to the inferences developed. Another extension to this project would be to carry out a factor analysis, indicating the independent latent variables that account for most of the variability of other variables in the data, reducing the dimensionality of the data while retaining interpretability.
\hfill\break
\newline
Finally, it is also worth readdressing the fact that the data was gathered over time, meaning the observations were not independent from each other. Further analysis of New.Bodyload could involve exploring serial correlations over time, spectral analysis to examine cyclic behavior and fitting more complex models that takes the time component of the data into account in a more mature manner.




